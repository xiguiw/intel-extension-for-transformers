:orphan:

:py:mod:`intel_extension_for_transformers.transformers.dpo_trainer`
===================================================================

.. py:module:: intel_extension_for_transformers.transformers.dpo_trainer


Module Contents
---------------

Classes
~~~~~~~

.. autoapisummary::

   intel_extension_for_transformers.transformers.dpo_trainer.DPOTrainer
   intel_extension_for_transformers.transformers.dpo_trainer.GaudiDPOTrainer




.. py:class:: DPOTrainer(model: Union[transformers.PreTrainedModel, torch.nn.Module] = None, ref_model: Optional[Union[transformers.PreTrainedModel, torch.nn.Module]] = None, beta: float = 0.1, args: transformers.TrainingArguments = None, data_collator: Optional[transformers.DataCollator] = None, label_pad_token_id: int = -100, padding_value: int = 0, train_dataset: Optional[datasets.Dataset] = None, eval_dataset: Optional[Union[datasets.Dataset, Dict[str, datasets.Dataset]]] = None, tokenizer: Optional[transformers.PreTrainedTokenizerBase] = None, max_length: Optional[int] = None, peft_config: Optional[Dict] = None, disable_dropout: bool = True)




   Initialize DPOTrainer, refer: https://github.com/huggingface/trl/blob/main/trl/trainer/dpo_trainer.py

   :param model: The model to train, preferably an `AutoModelForSequenceClassification`.
   :type model: `transformers.PreTrainedModel`
   :param ref_model: Hugging Face transformer model with a casual language modelling head.
                     Used for implicit reward computation and loss. If no
                     reference model is provided, the trainer will
                     create a reference model with the same architecture as the model to be optimized.
   :type ref_model: `PreTrainedModelWrapper`
   :param beta: The beta factor in DPO loss. Higher beta means less divergence from the initial policy.
   :type beta: `float`, defaults to 0.1
   :param args: The arguments to use for training.
   :type args: `transformers.TrainingArguments`
   :param data_collator: The data collator to use for training. If None is specified,
                         the default data collator (`DPODataCollatorWithPadding`) will be used
                         which will pad the sequences to the maximum length of the sequences in the batch,
                         given a dataset of paired sequences.
   :type data_collator: `transformers.DataCollator`
   :param label_pad_token_id: The label pad token id. This argument is required if you want to use the default data collator.
   :type label_pad_token_id: `int`, defaults to `-100`
   :param padding_value: The padding value. This argument is required if you want to use the default data collator.
   :type padding_value: `int`, defaults to `0`
   :param train_dataset: The dataset to use for training.
   :type train_dataset: `datasets.Dataset`
   :param eval_dataset: The dataset to use for evaluation.
   :type eval_dataset: `datasets.Dataset`
   :param tokenizer: The tokenizer to use for training. This argument is required if you want to use the default data collator.
                     The callbacks to use for training.
   :type tokenizer: `transformers.PreTrainedTokenizerBase`
   :param max_length: The maximum length of the sequences in the batch.
                      This argument is required if you want to use the default data collator.
   :type max_length: `int`, defaults to `None`
   :param peft_config: The PEFT configuration to use for training. If you pass a PEFT configuration,
                       the model will be wrapped in a PEFT model.
   :type peft_config: `Dict`, defaults to `None`
   :param disable_dropout: Whether or not to disable dropouts in `model` and `ref_model`.
   :type disable_dropout: `bool`, defaults to `True`

   .. py:method:: dpo_loss(policy_chosen_logps: torch.FloatTensor, policy_rejected_logps: torch.FloatTensor, reference_chosen_logps: torch.FloatTensor, reference_rejected_logps: torch.FloatTensor) -> Tuple[torch.FloatTensor, torch.FloatTensor, torch.FloatTensor]

      Compute the DPO loss for a batch of policy and reference model log probabilities.



   .. py:method:: dpo_forward(model: torch.nn.Module, batch: Dict[str, Union[List, torch.LongTensor]]) -> Tuple[torch.FloatTensor, torch.FloatTensor, torch.FloatTensor, torch.FloatTensor]

      Run the given model on the given batch of inputs, concatenating the chosen and rejected inputs together.

      We do this to avoid doing two forward passes, because it's faster for FSDP.


   .. py:method:: get_batch_metrics(model, batch: Dict[str, Union[List, torch.LongTensor]], train_eval: Literal[train, eval] = 'train')

      Compute the DPO loss and other metrics for the given batch of inputs for train or test.


   .. py:method:: log(logs: Dict[str, float]) -> None

      Log `logs` on the various objects watching training, including stored metrics.

      :param logs: The values to log.
      :type logs: `Dict[str, float]`



.. py:class:: GaudiDPOTrainer(model: Union[transformers.PreTrainedModel, torch.nn.Module] = None, ref_model: Optional[Union[transformers.PreTrainedModel, torch.nn.Module]] = None, beta: float = 0.1, args: transformers.TrainingArguments = None, data_collator: Optional[transformers.DataCollator] = None, label_pad_token_id: int = -100, padding_value: int = 0, train_dataset: Optional[datasets.Dataset] = None, eval_dataset: Optional[Union[datasets.Dataset, Dict[str, datasets.Dataset]]] = None, tokenizer: Optional[transformers.PreTrainedTokenizerBase] = None, max_length: Optional[int] = None, peft_config: Optional[Dict] = None, disable_dropout: bool = True)




   Initialize habana



