:py:mod:`intel_extension_for_transformers.transformers.pipeline`
================================================================

.. py:module:: intel_extension_for_transformers.transformers.pipeline

.. autoapi-nested-parse::

   Pipeline: import transformers.pipelines and support int8 model loading based on infer_framework_load_model.



Module Contents
---------------


Functions
~~~~~~~~~

.. autoapisummary::

   intel_extension_for_transformers.transformers.pipeline.infer_framework_load_model



.. py:function:: infer_framework_load_model(model, config: transformers.AutoConfig, model_classes: Optional[Dict[str, Tuple[type]]] = None, task: Optional[str] = None, framework: Optional[str] = None, **model_kwargs)

   Support int8 model loading based on infer_framework_load_model.

   :param model: the input model
   :type model: object
   :param config: AutoConfig object
   :type config: AutoConfig
   :param model_classes: model class. Defaults to None
   :type model_classes: Optional[Dict[str, Tuple[type]]], optional
   :param task: task name. Defaults to None
   :type task: Optional[str], optional
   :param framework: framework name. Defaults to None
   :type framework: Optional[str], optional

   :returns: A tuple framework, model.
   :rtype: Tuple


