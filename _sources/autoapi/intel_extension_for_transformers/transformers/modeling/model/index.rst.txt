:py:mod:`intel_extension_for_transformers.transformers.modeling.model`
======================================================================

.. py:module:: intel_extension_for_transformers.transformers.modeling.model

.. autoapi-nested-parse::

   OptimizedModel: provides the from_pretrained function.



Module Contents
---------------

Classes
~~~~~~~

.. autoapisummary::

   intel_extension_for_transformers.transformers.modeling.model.OptimizedModel




.. py:class:: OptimizedModel(*args, **kwargs)


   Provide the from_pretrained function.

   .. py:method:: from_pretrained(model_name_or_path: str, **kwargs)
      :classmethod:

      Instantiate a quantized pytorch model from a given Intel Neural Compressor (INC) configuration file.

      :param model_name_or_path: Repository name in the Hugging Face Hub or path to a local directory hosting the model.
      :type model_name_or_path: :obj:`str`
      :param cache_dir: Path to a directory in which a downloaded configuration should be cached if the standard cache should
                        not be used.
      :type cache_dir: :obj:`str`, `optional`
      :param force_download: Whether or not to force to (re-)download the configuration files and override the cached versions if
                             they exist.
      :type force_download: :obj:`bool`, `optional`, defaults to :obj:`False`
      :param resume_download: Whether or not to delete incompletely received file. Attempts to resume the download if such a file
                              exists.
      :type resume_download: :obj:`bool`, `optional`, defaults to :obj:`False`
      :param revision: The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
                       git-based system for storing models and other artifacts on huggingface.co, so ``revision`` can be any
                       identifier allowed by git.
      :type revision: :obj:`str`, `optional`

      :returns: Quantized model.
      :rtype: q_model



